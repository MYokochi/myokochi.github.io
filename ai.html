<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" href="assets/images/✨.ico">
    <title>Margarida Yokochi Portfolio</title>
    <meta name="description" content="Portfolio Test">
		<meta name="keywords" content="Design, Portfolio">
		<meta name="author" content="Margarida Yokochi">
    <link rel="stylesheet" href="properties.css">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <!-- Navbar -->
		<nav class="back">
      <a type="button" class="button"  href="index.html">
        <svg width="24" height="24" viewBox="0 0 24 24" class="icon" xmlns="http://www.w3.org/2000/svg">
          <path fill-rule="evenodd" clip-rule="evenodd" d="M21.001 12.5C21.001 12.0858 20.6652 11.75 20.251 11.75H3.75098C3.33676 11.75 3.00098 12.0858 3.00098 12.5C3.00098 12.9142 3.33676 13.25 3.75098 13.25H20.251C20.6652 13.25 21.001 12.9142 21.001 12.5Z"/>
          <path fill-rule="evenodd" clip-rule="evenodd" d="M11.0303 5.21968C10.7374 4.92678 10.2626 4.92678 9.96967 5.21968L3.21967 11.9697C2.92678 12.2626 2.92678 12.7374 3.21967 13.0303L9.96967 19.7803C10.2626 20.0732 10.7374 20.0732 11.0303 19.7803C11.3232 19.4874 11.3232 19.0126 11.0303 18.7197L4.81066 12.5L11.0303 6.28034C11.3232 5.98744 11.3232 5.51257 11.0303 5.21968Z"/>
          </svg>Portfolio          
      </a>
   </nav>

    <div class="content">
      <div>
        <h1>AI in 3D Product Configurators: Improving Product Customisation Experience</h1>
        <div>
          <p><b style="color: var(--accent);">Role:</b> Product Designer, UX Researcher<br><b style="color: var(--accent);">Timeline:</b>  April 2023 - July 2023</p>
        </div>
        
      </div>
      <img src="assets/ai/ai.png" class="image" width="720px" height="auto">
       
      <div class="section">
        <h2 id="overview">Overview</h2>
        <p>The goal of this initiative was to explore ways in which a large language model integration could improve the experience of our 3D product configurators.</p>
        <p>This case study covers the technical validation phase and early insights. We integrated GPT-4 using the OpenAI API. Our first learning sessions helped us get a better understanding of the potential and main challenges, as we searched for ways in which AI could add value to users.</p>
      </div>
      <div class="section">
        <h2 id="technical-validation">Technical Validation</h2>
        <p>Since OpenAI announced their API, everyone&#39;s been trying to figure out how to integrate AI in their products.</p>
        <p>There are a lot of unknowns when using new technologies and finding novel applications. From a technical, we had to find out if it was possible to configure a product using only natural language.</p>
        <p>For this first phase, we implemented a chat that opened independently from the existing interface. We wanted to test mainly functionality. The goal was to validate if all aspects of configuration could be changed using only the chat.</p>
        <img src="assets/ai/sketch.png" class="image" alt=""></p>
      </div>

      

      <div class="section">
        <h2 id="learnings-from-research">Learnings from Research</h2>
        <p>The first step was to validate if this was an idea worth pursuing. We wanted to get input from people as soon as possible to get an idea of real world applications of this feature. In the first iteration, we integrated a chatbot into an existing kitchen configurator. Users could ask questions or instruct it to make changes such as adding or removing components, changing the size of elements or applying another colour.</p>
        <p>We found that, although this conversational approach had a lot of potential especially when users struggled to accomplish some task, <b>the chatbot framing wasn&#39;t ideal as people had strong notions of this concept based on previous bad experiences.</b> Essentially, users were impressed by the functionality once we asked them to try the feature but didn&#39;t have the motivation to try it themselves. The first big insight was that the way the feature was framed wasn&#39;t perceived as desirable. </p>
        <ul>
        <li>Users associated the chatbot with bad experiences and limited capabilities  </li>
        <li>The functionality delivered more value when users were stuck or needed help to complete an action  </li>
        <li>The feature was not perceived as desirable  </li>
        <li>Users still wanted control and seeing the available options helped them model what was possible  </li>
        <li>Users expected more guidance from an assistant
        </li>
        </ul>
        <img src="assets/ai/chat.png" class="image" alt="">
      </div>

      

      <div class="section">
        <h2 id="rethinking-our-approach">Rethinking our Approach</h2>
        <p>Now that we knew we could deliver technically, we needed to find a way to add value with this feature. We made a plan to align the team but realised that business objectives and vision for the product weren’t clear. We decided to work on defining this vision first so we could better understand how to frame the feature in the context of the whole product.</p>
        <img src="assets/ai/chat_opened.png" class="image" alt="">
      </div>


      

      <div class="section">
        <h2 id="conclusion">Conclusion</h2>
        <p>Starting with a solution is often dangerous because you run the risk of building something that doesn&#39;t solve a real problem or user need. It is exciting to use new technologies and often necessary to create proofs of concept for technical validation. It was important for our team to see users be impressed by the capabilities of the feature and still say – it&#39;s cool but I don&#39;t really need it. Now we can focus on uncovering actual user needs and build a solution that delivers value.</p>
      </div>
    </div>

    <footer>
      <div>
        <p class="footer">Margarida Yokochi &#x2022 May 2023</p>
      </div>
  </footer>

  </body>
</html>